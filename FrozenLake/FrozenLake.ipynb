{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f0882948",
   "metadata": {},
   "source": [
    "# Jane Slagle\n",
    "# CS 138 RL - Programming Assignment 3\n",
    "# FrozenLake.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c2d56731",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "from gymnasium.envs.toy_text.frozen_lake import generate_random_map\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef9d9327",
   "metadata": {},
   "source": [
    "## Set up the frozen lake environment from gym using a randomly generated grid to respresent the lake as well as perform various other set up tasks including getting the number of states, actions out and initializing all needed parameters for running through the algorithms:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "547da644",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SFFH\n",
      "FFFF\n",
      "FFFF\n",
      "FFFG\n"
     ]
    }
   ],
   "source": [
    "#generate a random 4x4 grid map to create frozen lake env with\n",
    "rand_map = generate_random_map(size=4)\n",
    "\n",
    "#set up frozen lake env from gym\n",
    "env = gym.make(\"FrozenLake-v1\", desc=rand_map, is_slippery = False)\n",
    "\n",
    "#print out what 4x4 frozen lake grid env looks like\n",
    "for row in rand_map:\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4f07a593",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num states: 16\n",
      "num actions: 4\n"
     ]
    }
   ],
   "source": [
    "#get number states, actions out of env\n",
    "num_states = env.observation_space.n\n",
    "num_acts = env.action_space.n\n",
    "\n",
    "print(\"num states: \" + str(num_states))\n",
    "print(\"num actions: \" + str(num_acts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8d4215f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#need gamma for algorithm equations -- use 0.9 as gamma bc common value for it based off recorded lectures\n",
    "gamma = 0.9 \n",
    "\n",
    "#define all params will need for when actually run through algorithms\n",
    "num_eps = 5000        #num training episodes\n",
    "max_steps = 100       #max steps per episode, if dont have then never reach goal state when run algors\n",
    "\n",
    "#init val funcs V for 2 algor (7.13 + 7.2 and 7.1 + 7.9) are running here\n",
    "V_algor_1 = np.zeros(num_states)  #let equations 7.13 + 7.2 be rep as algor_1\n",
    "V_algor_2 = np.zeros(num_states)  #let equations 7.1 + 7.9 be rep as algor_2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fb3f4d3",
   "metadata": {},
   "source": [
    "## Define helper functions for executing the 2 algorithms, including generating data from each episode, specifying the behavior and target policies for the algorithms, and calculating the importance sampling ratio with those policies required for the algorithm equations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7226650c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate tuple of info for each episode using inputted given policy\n",
    "def gen_eps(env, policy, max_steps):\n",
    "    #each eps = tuple of (state, act, reward, next state, done) add done to each eps tuple bc will want\n",
    "    #to use it to get results from running algorithm at end\n",
    "    all_eps = []            #store all data from each episode\n",
    "    state = env.reset()     #make sure the env is reset for each episode\n",
    "    \n",
    "    #limit each episode run to number of max steps\n",
    "    for _ in range(max_steps):\n",
    "        act = policy()      #simluate taking action in each episode\n",
    "        \n",
    "        #now get info out from taking action in episode!\n",
    "        #env.step method takes act just took + returns the next state, reward, if done + other info\n",
    "        next_state, reward, done, info = env.step(act)\n",
    "        all_eps.append((state, act, reward, next_state, done))  #add this episode to list of all episodes\n",
    "        \n",
    "        state = next_state  #update state for next step take\n",
    "        \n",
    "        #use done value just got out to see if have reached terminal state. if have, then exit bc \n",
    "        #means no more episodes to generate data for!\n",
    "        if done:\n",
    "            break\n",
    "            \n",
    "    return all_eps  #return info from episodes just found!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "80a7ea5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#both algor have rho in them = importance sampling ratio (eq 7.10) so define that!\n",
    "#need behavior, target policies for calcuating importance sampling ratio!\n",
    "\n",
    "#make behavior policy random policy, selects action to take by randomly choosing from all actions have\n",
    "def b_policy():\n",
    "    return np.random.choice(num_acts)\n",
    "\n",
    "#make target policy greedy policy, will select action to take by choosing one w/ best Q func act val \n",
    "def t_policy(state, V):\n",
    "    Q = []    #store all Q act values\n",
    "    \n",
    "    #loop through all actions have + find all act vals for each state\n",
    "    for act in range(num_acts):\n",
    "        env.reset()   #make sure env is reset for each action\n",
    "        \n",
    "        #simulate env step, get all info want out of it\n",
    "        next_state, reward, done, info = env.step(act)\n",
    "        \n",
    "        #calc Q act val where Q formula = immed. reward + gamma * future reward where future reward is\n",
    "        #from the value func V (V is inputted in func, woohoo!)\n",
    "        q_val = reward + (gamma * V[next_state])\n",
    "        Q.append(q_val)\n",
    "        \n",
    "    #return act w/ highest expected Q val (bc greedy approach)\n",
    "    return np.argmax(Q)\n",
    "\n",
    "#now use those policy funcs here! = exactly equation 7.10 given in book\n",
    "def import_samp_ratio(eps, t_policy, b_policy, V):\n",
    "    #store all pi/b ratios for each time step in eps so that can take product of them all at end\n",
    "    rho = []\n",
    "    \n",
    "    #loop over each (state, act) pair in the eps\n",
    "    #eps returns (state, act, reward, next state, done state) tuple but only care about state, act here\n",
    "    for state, act, reward, next_state, done in eps:\n",
    "        #for each (state, act) pair in eps, need calc rho val\n",
    "        #pi = prob take act a under target policy at state s, so 1 if acts are same, else 0\n",
    "        if act == t_policy(state, V):\n",
    "            pi = 1.0\n",
    "        else:\n",
    "            pi = 0.0\n",
    "            \n",
    "        #b = 1/num acts for uniform rand behavior policy\n",
    "        b = 1.0 / num_acts\n",
    "        \n",
    "        rho.append(pi / b)\n",
    "        \n",
    "    #product runs over all time steps in eps\n",
    "    return np.prod(rho)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b8e6b06",
   "metadata": {},
   "source": [
    "## Simulate running through algorithm 1 (equations 7.13 + 7.2):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f44d51d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#algor 1: equations 7.13 + 7.2 from book - corresponds with V_algor_1\n",
    "\n",
    "def run_algor_1(env, num_eps, gamma):\n",
    "    reached_goal_algor_1 = 0      #keep count of number times agent reached goal state on frozen lake grid\n",
    "    didnt_reach_goal_algor_1 = 0  #keep count number times agent didnt reach goal state, from either just\n",
    "                                  #not reaching goal state or from falling in hole\n",
    "    steps_taken_algor_1 = []      #keep track num steps took to reach goal state, will store num \n",
    "                                  #steps for each episode\n",
    "                \n",
    "    #run through for all episodes have\n",
    "    for _ in range(num_eps):\n",
    "        #actually generate episode for each episode loop through!\n",
    "        #use behavior policy to generate data for each episode\n",
    "        eps = gen_eps(env, b_policy, max_steps) \n",
    "        \n",
    "        #find G in equation 7.13, store as array bc compute G_t:h based on G_t+1:h which means need\n",
    "        #compute next value before compute current one and if store as array then can do this!\n",
    "        G = np.zeros(len(eps))\n",
    "        \n",
    "        #find equation 7.13 1st bc it calcs G and need G for equation 7.2\n",
    "        #need next step to find current step so need work through time steps bwards\n",
    "        #specifically: need start at end of episode + work bwards from there, from 2nd to last time step\n",
    "        #bc at last time step, there = no next time step after it\n",
    "        for t in range(len(eps) -2, -1, -1):\n",
    "            #get importance sampling ratio value out, want for current time step t's eps\n",
    "            rho_t = import_samp_ratio([eps[t]], t_policy, b_policy, V_algor_1)\n",
    "            \n",
    "            #get all of info for specific episode time step out so that can use it!!!\n",
    "            state, act, reward, next_state, done = eps[t]\n",
    "            \n",
    "            #equation 7.13:\n",
    "            G[t] = rho_t * (reward + gamma * G[t+1]) + (1 - rho_t) * V_algor_1[state]\n",
    "            \n",
    "        #G vals computed now, so can use them to update val func w/ equation 7.2!!!\n",
    "        for t in range(len(eps)):\n",
    "            #get all info out of eps\n",
    "            state, act, reward, next_state, done = eps[t]\n",
    "            \n",
    "            #update val func w/ equation 7.2\n",
    "            V_algor_1[state] += gamma * (G[t] - V_algor_1[state])\n",
    "            \n",
    "            #check if in terminal state (if eps over)\n",
    "            #if eps is over, check if it reached goal or not\n",
    "            if done:\n",
    "                if reward == 1:\n",
    "                    #then means reached goal state - woohoo!\n",
    "                    reached_goal_algor_1 += 1\n",
    "                    steps_taken_algor_1.append(t+1) #keep track of steps taken to reach goal so that \n",
    "                                                    #can get their count in results\n",
    "                elif reward == 0:\n",
    "                    #then means either didnt reach goal or fell in hole... boooooo\n",
    "                    didnt_reach_goal_algor_1 += 1\n",
    "\n",
    "                break #eps ends when are in terminal state so break\n",
    "    \n",
    "    #get results from running algor out\n",
    "    get_results(steps_taken_algor_1, reached_goal_algor_1, didnt_reach_goal_algor_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe694621",
   "metadata": {},
   "source": [
    "## Simulate running through algorithm 2 (equations 7.9 + 7.1):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f1374b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#algor 2: equations 7.9 + 7.1 from book - corresponds with V_algor_2\n",
    "\n",
    "def run_algor_2(env, num_eps, gamma):\n",
    "    reached_goal_algor_2 = 0      #keep count of number times agent reached goal state on frozen lake grid\n",
    "    didnt_reach_goal_algor_2 = 0  #keep count number times agent didnt reach goal state, from either just\n",
    "                                  #not reaching goal state or from falling in hole\n",
    "    steps_taken_algor_2 = []      #keep track num steps took to reach goal state, will store num \n",
    "                                  #steps for each episode\n",
    "        \n",
    "    #run through for all episodes have\n",
    "    for _ in range(num_eps):\n",
    "        #actually generate episode for each episode loop through!\n",
    "        #use behavior policy to generate data for each episode\n",
    "        eps = gen_eps(env, b_policy, max_steps) \n",
    "        \n",
    "        #loop through all time steps for each indiv eps and simulate agent navigating over frozen lake!!!\n",
    "        for t in range(len(eps)):\n",
    "            #get all of the info for specific episode time step out so that can use it!!!\n",
    "            state, act, reward, next_state, done = eps[t]\n",
    "            \n",
    "            #compute G = equation 7.1 from book AHHHH!!!\n",
    "            #find 7.1 before 7.9 bc use 7.1 in 7.9\n",
    "            G = 0\n",
    "            #start from current time step t, run through until reach end of episode\n",
    "            for k in range(t, len(eps)):\n",
    "                G += gamma ** (k - t) * reward \n",
    "                \n",
    "            #get importance sampling ratio so that can use it in equation 7.9\n",
    "            rho_t = import_samp_ratio([eps[t]], t_policy, b_policy, V_algor_2)\n",
    "            \n",
    "            #update val func using equation 7.9 from book + using G just found w/ equation 7.1\n",
    "            V_algor_2[state] += rho_t * (G - V_algor_2[state])\n",
    "            \n",
    "            #check if in terminal state (if eps over)\n",
    "            #if eps is over, check if it reached goal or not\n",
    "            if done:\n",
    "                if reward == 1:\n",
    "                    #then means reached goal state - woohoo!\n",
    "                    reached_goal_algor_2 += 1\n",
    "                    steps_taken_algor_2.append(t+1) #keep track of steps taken to reach goal so that \n",
    "                                                    #can get their count in results\n",
    "                elif reward == 0:\n",
    "                    #then means either didnt reach goal or fell in hole... boooooo\n",
    "                    didnt_reach_goal_algor_2 += 1\n",
    "\n",
    "                break #eps ends when are in terminal state so break\n",
    "       \n",
    "    #get results from running algor out\n",
    "    get_results(steps_taken_algor_2, reached_goal_algor_2, didnt_reach_goal_algor_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0f817d6",
   "metadata": {},
   "source": [
    "## Get the results out from running the 2 algorithms:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ddcaeb9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print out results want: avg num steps took to reach goal state, percentage time reached goal state,\n",
    "#percentage time fell in hole or didnt reach goal state when simulating across all episodes\n",
    "\n",
    "def get_results(steps_taken, goal_count, not_goal_count):\n",
    "    #take mean of steps_taken bc it is an array for each episode so get average of num steps taken across\n",
    "    #all episodes\n",
    "    avg_steps_taken = np.mean(steps_taken)\n",
    "    print(\"The goal state was reached in an average of \" + str(avg_steps_taken) + \" steps.\")\n",
    "    \n",
    "    #get percentage of number times goal state was reached out of all episodes\n",
    "    goal_reached_percent = (goal_count / num_eps) * 100\n",
    "    print(\"The goal state was reached \" + str(goal_reached_percent) + \" percent of the time.\")\n",
    "    \n",
    "    #get percentage of number times goal state was not reached out of all episodes\n",
    "    goal_not_reached_percent = (not_goal_count / num_eps) * 100\n",
    "    print(\"The goal state was not reached \" + str(goal_not_reached_percent) + \" percent of the time, either from ending the episode in a frozen state or from falling into a hole.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22a8b528",
   "metadata": {},
   "source": [
    "## Results from running the first algorithm (7.13 + 7.2):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "cb0d9498",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The goal state was reached in an average of 30.657945736434108 steps.\n",
      "The goal state was reached 41.28 percent of the time.\n",
      "The goal state was not reached 58.720000000000006 percent of the time, either from ending the episode in a frozen state or from falling into a hole.\n"
     ]
    }
   ],
   "source": [
    "#run advanced algorithm to get results\n",
    "run_algor_1(env, num_eps, gamma)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ebd1423",
   "metadata": {},
   "source": [
    "## Results from running the second algorithm (7.9 + 7.1):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "203f62b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The goal state was reached in an average of 30.71672828096118 steps.\n",
      "The goal state was reached 43.28 percent of the time.\n",
      "The goal state was not reached 56.720000000000006 percent of the time, either from ending the episode in a frozen state or from falling into a hole.\n"
     ]
    }
   ],
   "source": [
    "#run simple algorithm to get results\n",
    "run_algor_2(env, num_eps, gamma)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deb00c95",
   "metadata": {},
   "source": [
    "## Additional Question - running the same frozen lake environment with SARSA on-policy TD control algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "df7aa46b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SARSA on-policy TD control algorithm from Section 6.4, page 130 in book\n",
    "#exactly follow the pseudocode given in book!!!\n",
    "\n",
    "#normal values for both wanted params with algorithm according to recorded lectures\n",
    "alph = 0.1\n",
    "epsilon = 0.1   \n",
    "\n",
    "def run_sarsa_algor(env, num_eps, max_steps, gamma, alph, epsilon):\n",
    "    Q_sarsa = np.zeros((num_states, num_acts))  #init Q(s,a) for all states, actions\n",
    "    \n",
    "    reached_goal_sarsa = 0      #keep count of number times agent reached goal state on frozen lake grid\n",
    "    didnt_reach_goal_sarsa = 0  #keep count number times agent didnt reach goal state, from either just\n",
    "                                #not reaching goal state or from falling in hole\n",
    "    steps_taken_sarsa = []      #keep track num steps took to reach goal state, will store num \n",
    "                                #steps for each episode\n",
    "        \n",
    "    #loop over all episodes have\n",
    "    for _ in range(num_eps):\n",
    "        state = env.reset()     #init state S\n",
    "        \n",
    "        #choose action A from state S using eps-greedy policy derived from Q_sarsa\n",
    "        if np.random.uniform(0,1) < epsilon:  \n",
    "            #means want explore, can call b_policy() func wrote for this!\n",
    "            act = b_policy()\n",
    "        else:\n",
    "            #means want exploit, choose the act w/ max Q func val here\n",
    "            act = np.argmax(Q_sarsa[state])\n",
    "            \n",
    "        #loop for each step of episode (each episode is at most max_steps amount long):\n",
    "        for t in range(max_steps):\n",
    "            #take action A, observation R, next state S'\n",
    "            next_state, reward, done, info = env.step(act)\n",
    "            \n",
    "            #choose next action from next state using policy derived from Q_sarsa (eps-greedy)\n",
    "            if np.random.uniform(0,1) < epsilon:\n",
    "                #mans explore, call b_policy() func wrote for tihs!\n",
    "                next_act = b_policy()\n",
    "            else:\n",
    "                #means want exploit, choose act w/ max Q func val here\n",
    "                next_act = np.argmax(Q_sarsa[next_state])\n",
    "            \n",
    "            #update Q(s,a) using formula given in book\n",
    "            #Q(s,a) = Q(s,a) + alpha[R + gamma Q(next state, next act) - Q(s,a)]\n",
    "            Q_sarsa[state, act] += alph * (reward + (gamma * Q_sarsa[next_state, next_act] - Q_sarsa[state, act]))\n",
    "            \n",
    "            #update S + update A for next run through loop\n",
    "            #S = next state, A = next action\n",
    "            state = next_state\n",
    "            act = next_act\n",
    "            \n",
    "            #keep doing this until the state is terminal so check if the state is or not!\n",
    "            #then figure out if reached goal or not for getting results out\n",
    "            if done:\n",
    "                if reward == 1:\n",
    "                    #then means reached goal state - woohoo!\n",
    "                    reached_goal_sarsa += 1\n",
    "                    steps_taken_sarsa.append(t+1) #keep track of steps taken to reach goal so that can \n",
    "                                                  #get their count in results\n",
    "                elif reward == 0:\n",
    "                    #then means either didnt reach goal or fell in hole... boooooo\n",
    "                    didnt_reach_goal_sarsa += 1\n",
    "                    \n",
    "                break #eps ends when are in terminal state so break\n",
    "                \n",
    "    #get results from running SARSA algor out\n",
    "    get_results(steps_taken_sarsa, reached_goal_sarsa, didnt_reach_goal_sarsa)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "348a7b2d",
   "metadata": {},
   "source": [
    "## Results from running SARSA:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "5331d6ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The goal state was reached in an average of 7.034659557013946 steps.\n",
      "The goal state was reached 97.52 percent of the time.\n",
      "The goal state was not reached 2.48 percent of the time, either from ending the episode in a frozen state or from falling into a hole.\n"
     ]
    }
   ],
   "source": [
    "#run SARSA algorithm to get results\n",
    "run_sarsa_algor(env, num_eps, max_steps, gamma, alph, epsilon)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
